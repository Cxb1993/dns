\documentclass[12pt]{article}
\usepackage{amssymb,latexsym,amsmath}

\newif\ifpdf
\ifx\pdfoutput\undefined
\pdffalse % we are not running PDFLaTeX                                         
\else
\pdfoutput=1 % we are running PDFLaTeX                                          
\pdftrue
\fi
\ifpdf
\usepackage[pdftex]{graphicx}
\else
\usepackage{graphicx}
\fi
\ifpdf
\DeclareGraphicsExtensions{.pdf, .jpg, .tif}
\else
\DeclareGraphicsExtensions{.eps, .jpg}
\fi



\textwidth = 6.5 in
\textheight = 9 in
\oddsidemargin = 0.0 in
\evensidemargin = 0.0 in
\topmargin = 0.0 in
\headheight = 0.0 in
\headsep = 0.0 in
\parskip = 0.2 in
\parindent = 0.0 in


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


\newcommand{\khat}{\hat{\mathbf k}}
\newcommand{\uv}{\mathbf u}
%\newcommand{\vor}{\mathbf \omega}
\newcommand{\vor}{\mathbf w}
\newcommand{\n}{\mathbf n}
\newcommand{\f}{\mathbf f}


\newcommand{\grad}{\nabla}
\newcommand{\curl}{\grad \times}
\renewcommand{\div}{\grad \cdot}

\DeclareMathOperator{\Span}{span}


\title{Outline of Pseudo-Spectral Method used in the Sandia/LANL DNS code}
\author{Mark Taylor}

\begin{document}
\maketitle
\tableofcontents

\section{Introduction}
The Sandia/LANL DNS code solves viscous fluid dynamics equations in a
rectangular domain (2D or 3D).  It has many options for the equations
(Navier-Stokes, Boussinesq, shallow water) discretization (pseudo spectral, 
finite differences, hybrid, RK4, RK2, stabilized leapfrog).  This document
describes the RK4 pseudo-spectral model for modeling isotropic homogeneous
turbulence (incompressible Navier-Stokes) in a 3D triply periodic box.

The code is documented in 
\begin{enumerate}
\item Taylor, Kurien and Eyink, Phys. Rev. E 68, 2003. 
\item Kurien and Taylor, Los Alamos Science 29, 2005. 
\end{enumerate}
Results from this code have also been used in four additional journal
publications.  Our largest simulation to date is 4 eddy turnover
times of decaying turbulence at $2048^3$.  We use MPI and allow
for arbitrary 3D domain decomposition, (which includes slabs,
pencils or cubes0.  For a grid of size $N^3$, it
can run on up to $N^3/8$ processors.  It has 
been run on as many as 6144 processors with grids as large
as $4096^3$.

We solve the equations in a square box of side length 1.
The equations are
\[
\frac{ \partial  \uv }{\partial t}  + \vor  \times \uv + 
\grad p   = \nu \Delta \uv + \f
\]
\[
\div \uv = 0
\]
with vorticity $\vor = \curl \uv$, forcing function $f$, and $p$ is the modified pressure
given by $p = .5 \uv^2 + p'$ (with $p'$ being the actual Navier-Stokes pressure).
We use a pressure projection method, which determines
$\grad p$ so that the solution remains
divergence free.  In this method, the continuity equation $\div \uv = 0$
is eliminated by taking
\[
 p =  \Delta^{-1} \div \left( -\vor \times \uv  + \f \right)
\]




\section{Outline of Numerical Method}
With the pressure projection method, the Navier-Stokes equations
can be written as a single prognostic equation
\[
\frac{ \partial  \uv }{\partial t} = 
  - \vor  \times \uv + 
\grad \Delta^{-1} \div \left( \vor \times \uv  -\f \right) +  \nu \Delta \uv + \f
\]
The terms on the RHS can all be explictly computed via the FFT, and
thus the equation can be thought of as a system of ODE's for the
Fourier coefficients of $\uv$.  

\subsection{RK4} 

We use the classic 4th order Runge Kutta scheme with an explicit
treatment of diffusion.  This requires, for each timestep, 4 evaluations 
of the right-hand-side (RHS) of the Navier-Stokes equation.
All that is required to advance the solution in time is 
simple linear combinations of these 4 RHS evaluations.  So the timestepping
procedure (not counting the cost to compute the RHS) is a few loops
with no communication at a cost of $O(N^3)$.  

\subsection{Evaluation of RHS}
The RHS of the Navier Stokes equations is
\[
  - \vor  \times \uv + 
\grad \Delta^{-1} \div \left( \vor \times \uv  -\f \right) +  \nu \Delta \uv + \f
\]
Denote the Fourier coefficients of $\uv$ by $\hat \uv$, and
let $\n = \vor \times \uv$.   
We advance in time $\hat \uv$, so for each
of the 4 Runge-Kutta stages we must compute the Fourier coefficients 
of the RHS.  Starting with $\hat \uv$, the steps are
\begin{enumerate}
\item  $\uv$ = iFFT($\hat \uv$).  Cost: 3D inverse FFT
\item  $\hat \vor = \curl \hat \uv$.  (single loop, no communication, cost: $O(N^3)$)
\item  $\vor$ = iFFT($\hat \vor$).  Cost: 3D inverse FFT 
\item  Compute $\n = \vor \times \uv$.  (single loop, no communication, cost: $O(N^3)$)
\item  $\hat\n$=FFT($\n$).  Cost: 3D FFT
\item  Compute $\hat \f$.  Cost: negligable if forcing implemented in Fourier space.
\item  Compute Fourier coefficients of remaining linear terms,
$\grad \Delta^{-1} \div (\hat\n - \hat \f) +  \nu \Delta \hat \uv$ (two loops, no communication, cost $O(N^3)$). 
\item  Add intermediate results to form Fourier coefficients of the RHS.  Cost: free
(can be combined with the previous step).
\end{enumerate}




\section{Complexity}

We will ignore the on processor loops, since they require no
communication and their total cost is $O(N^3)$, while the cost of the
FFT is $O(N^3 \log N)$.   Thus the cost of the method is all in
the cost of the 3 FFTs.  Each FFT is applied to a 3 component
vector field, so the total cost is given by 9 scalar 3D FFTs.  
All the parallel communication is hidden in the FFTs. 

The FFT looks something like this.  We start with a scalar variable
like $p$, (which could also be one component of the velocity vector $\uv$)
stored with an x-pencil decomposition:
\begin{enumerate}
\item Compute 1D FFT of $p$ ($N^2$ FFTs in the x-direction)
\item Transpose from x-pencil to y-pencil decomposition
\item Compute 1D FFT of $p$ ($N^2$ FFTs in the y-direction)
\item Transpose from y-pencil to z-pencil decomposition
\item Compute 1D FFT of $p$ ($N^2$ FFTs in z-direction)
\end{enumerate}
and end up with $\hat p$, stored with a z-pencil decomposition.
The Inverse FFT simply reverses the above steps.  

To estimate the cost of this algorithm, we just need a model for the
transpose operation and $N^2$ simultaneous 1D FFTs.  
Many codes first use a real-to-complex FFT, followed by
complex-to-complex FFTs for the last two (of size N/2).  
Our code uses only real-to-real FFTs, always of length N (the cost
is identical).

The total cost of of doing three of the 3D FFTs is thus:
\begin{enumerate}
\item 27 $N^2$ 1D real-to-real FFTs of length N
\item 3 x-pencil to y-pencil transposes of size $N^3$.  
\item 3 y-pencil to z-pencil transposes of size $N^3$.  
\item 6 z-pencil to y-pencil transposes of size $N^3$.  
\item 6 y-pencil to x-pencil transposes of size $N^3$.  
\end{enumerate}


\subsection{Vorticity Evaluation Trick}


There is a trick one can use to eliminate one transpose.
In Step 3 above, we only compute the second and third components
of $\vor$.  We use the notation
\[
 \vor =
\begin{pmatrix}
 \uv_{3,2} - \uv_{2,3}  \\ 
 \uv_{1,3} - \uv_{3,1}  \\ 
 \uv_{2,1} - \uv_{1,2}  
\end{pmatrix}
\]
where $\uv_{i,j}$ is the $j$'th derivative of the $i$'th component.
A small savings can be had if the third component of the vorticity, $\vor_3$   is
instead computed during the step when $\uv$ is computed via the iFFT
from $\hat \uv$.  While
computing the inverse FFT of $\uv$, one can also efficiently compute 
$\uv_{2,1}$ and $\uv_{1,2}$.  This requires the same number of 1D FFTs and
y-to-x pencil transposes as would be required to compute $\vor_1$,
but it has the advantage of requiring one less z-to-y pencil
transpose.  With a slab decompostion, this will reduce the amount
of communication by 1/9. With a pencil decompostion, the reduction
is 1/18.  

\subsection{Other Savings}

I know of one other trick which can be used to recover the vorticity
in grid space from the intermediate products of the $\uv = \text{iFFT}(\hat\uv)$ 
operation.  It will reduce the number of 1D FFTs from
27 to 24, but requires doing each component of the 
$\uv = \text{iFFT}(\hat\uv)$  operation in a different order.
(i.e. for one component we need to do the first the iFFT in the 
z-direction, then
the y-direction and then the x-direction, but for another component
we must first do the x-direction, then the y-direction and then
the z-direction.)
This will introduce additional distributed transposes which I
conjecture will make the algorithm less efficient for large
processor counts.

And of course there could be many other tricks I dont know about!


\subsection{Code Specific Transpose Details}
Our code actually allows for a full 3D data decomposition
(not that useful for the full pseudo spectral code, but useful
for some finite difference/spectral hybrids).  Because of this,
the FFT involves two extra steps.  The code
is defined to have a {\em reference} decomposition which
can be slabs, pencils or cubes.  The actual FFT algorithm
(in the code, see the subroutines 
\texttt{zx\_ifft3d()} and \texttt{zx\_fft3d()} in \texttt{fftops.F90} ) 
looks like this:

\begin{enumerate}
\item Compute 1D FFT of $p$ ($N^2$ FFTs in the x-direction, on processor)
\item Transpose from x-pencil to reference decomposition 
\item Transpose from reference to y-pencil decomposition
\item Compute 1D FFT of $p$ ($N^2$ FFTs in the y-direction, on processor)
\item Transpose from y-pencil to reference decomposition
\item Transpose from reference to z-pencil decomposition
\item Compute 1D FFT of $p$ ($N^2$ FFTs in z-direction, on processor)
\end{enumerate}


This algorithm contains two extra steps.  The most efficient
configuration is if the reference decomposition happens to be x-y slabs.
In that case, then the two extra steps (and the x-pencil transposes)
are all done on-processor (simple memory copies).
For performance at high resolution, one is usually required to
use y-pencils to allow for more parallization.  
If the reference decomposition is y-pencils, then again the
two extra steps are on-processor memory copies with only 
a small cost.  They also serve to arrange the data so all
FFTs are done with a stride of 1 and thus their cost is offset
by allowing for a more efficient FFT.

Thus with our code, for efficiency, it is best to use a reference 
decomposition of y-pencils (requiring two distributed tranposes for
each FFT) or x-y slabs (requiring one distributed transpose for each FFT).  
Using x-pencils or z-pencils  for the reference decomposition is very inefficient.  




\subsection{Cost of a single transpose}

Take a computational grid of size $N^3$, and assuming the code is
using a reference decompostion of y-pencils, so the domain
decompostion looks like $N_1 \times 1 \times N_2$, with the total
number of processes $N_p = N_1 N_2 $.   Then the distributed
tranposes are just the x-pencil to/from y-pencil and y-pencil to/from
z-pencil routines.  

For the x-pencil to/from y-pencil transpose the cost is
\begin{enumerate}
\item MPI\_Isend:  $N_1 -1$ messages of size $N^3/(N_1^2 N_2)$
\item MPI\_Irecv:  $N_1 -1$ messages of size $N^3/(N_1^2 N_2)$
\end{enumerate}

For the y-pencil to/from z-pencil transpose the cost is
\begin{enumerate}
\item MPI\_Isend:  $N_2 -1$ messages of size $N^3/(N_2^2 N_1)$
\item MPI\_Irecv:  $N_2 -1$ messages of size $N^3/(N_2^2 N_1)$
\end{enumerate}


Each scalar tranpose requires the network transmit a total just
shy of $2N^3$ real*8 numbers.  


\subsection{Total cost of the code}

The total cost of the code, for 1 Runge-Kutta stage,
in terms of communication and flops,
and assuming a y-pencil reference decomposition 
(with the total number of processes $N_p = N_1 N_2$) and a grid of $N^3$
can thus be estimated as

\begin{enumerate}
\item $27N^2$ 1D real-to-real FFTs of length N
\item Each process: MPI\_Isend:  $9(N_1 -1)$ messages of size $8N^3/(N_1^2 N_2)$ bytes.
\item Each process: MPI\_Irecv:  $9(N_1 -1)$ messages of size $8N^3/(N_1^2 N_2)$ bytes.
\item Each process: MPI\_Isend:  $8(N_2 -1)$ messages of size $8N^3/(N_2^2 N_1)$ bytes.
\item Each process: MPI\_Irecv:  $8(N_2 -1)$ messages of size $8N^3/(N_2^2 N_1)$ bytes.
\end{enumerate}

For the total ammount of bytes that must be sent over the network, add
the above numbers and multiply by the number of processes to get: $34
\cdot 8 N^3$ bytes.  (When using a slab decomposition, this is $16
\cdot 8 N^3$.)  These numbers are per Runge-Kutta stage.  One timestep
requires 4 stages, so for the total per timestep we need to multiply
by 4.

I've also measured the FLOP count per timestep with the following
procedure:  Using a hardware counter, measure the FLOP count
for a run with 5 timesteps and 4 timesteps, and take their
difference, to get the FLOP count per timestep.  
I did this using the FFT91 Fourier Transform, with resolutions
N=16, 32, 64,128 and 256.  Curve fitting the data resultant data
gave the following:  

{\bf FLOP count estimate per timestep for $N^3$ grid, N a power of 2:}
\[
4(  297 N^3 + 2.28 27 N^3 \log_2(N) )
\]

Repeating this procedure with resolutions of N=12, 24, 48, 96 and 192,
gave

{\bf FLOP count estimate per timestep for $N^3$ grid, N a power of 2 and one power of 3:}
\[
4(  340 N^3 + 2.24 27 N^3 \log_2(N) )
\]

Since there are 4 Runge-Kutta stage per timestep, the quantity
in brackets can be considered the cost for each stage.
These estimates, for the ranges of N given, were accurate to better
than 1\%.  



\section{Memory requirements}

The storage requirements of the code, for a grid of size $N^3$ 
are approximatly 18 real*8 arrays of size $N^3$, for a total
of $144 N^3$ bytes.  

The memory usage is fully scalable.  If there are $N_p$ processes,
each one requires no more than $144 N^3/N_p$ bytes.  





\section{Dealiasing}

There are three types of dealiasing commonly used in these codes.
\begin{enumerate}
\item Phase shifting
\item 2/3 rule
\item Spherical
\end{enumerate}

Dealiasing is necessary when computing the nonlinear term 
$\vor \times \uv$.  On a computational grid of size $N^3$, the 
Fourier coefficient $\hat \uv(l,m,n)$ has a wave number 
given by $2 \pi (l,m,n)$.  (We have the $2\pi$ factor since our
domain is of side length 1.)  The maximum wave number is $2 \pi N/2$.

\subsection{2/3 rule}
The 2/3 rule sets to zero all coefficients all wave numbers larger than
$(2 \pi) (2/3)(N/2)$. To be precise, we set to zero (or do not even compute)
\[
\hat \uv(l,m,n) = 0 \qquad  \text{ for } l > \frac{N}{3}, \quad m > \frac{N}{3}, \text{ or }  n > \frac{N}{3}.
\]
With this type of dealiasing, we define the maximum wave number
in our simulation to be $k_\text{max} = 2 \pi N/3$

\subsection{Spherical dealiasing}
Spherical dealiasing does not exactly remove all aliasing errors
from the nonlinear term.  It instead relies on the fact that in
fully developed turbulence, the energy will decay at a rate of
$k^{-5/3}$.  (where $k$ is the spherical wave number, $k = 2 \pi \|(l,m,n)\|$. )
Under this assumption, bounds on the aliasing errors can be derived
suggesting it is sufficient to dealias using:
\[
\hat \uv(l,m,n) =0 \qquad \text{ for } \sqrt{l^2 + m^2 + n^2} > \frac{\sqrt{2}}{3} N
\]
With this type of dealiasing, we define the maximum wave number
in our simulation to be $k_\text{max} = 2 \pi N\sqrt{2}/3$

\subsection{Phase shifting}
Phase shifting is an exact dealiasing which does not result in any
loss of resolution ($k_\text{max} = \pi N$) but requires additional
3D FFTs.  We have not implemented phase shifting.



\section{Simulations of Turbulence}


\subsection{Relevant Parameters}
We give some of the parameters used to discribe turbulent flow.

Velocity
\[
\uv = \left( u_1, u_2, u_3 \right)
\]

Kinetic energy
\[
k=\frac12 < \uv \cdot \uv > = \frac12   \sum_i < u_i^2 > 
\]
For the isotropic case, we can take $k=\frac32 <u_1^2 >$.
Dissipation rate
\[
\epsilon = \nu \sum_i < \grad u_i \cdot \grad u_i >
\]
Integral-like length scale
\[
L = k^{3/2} \epsilon^{-1}
\]
Kolmogorov length scale
\[
\eta = {\nu^{\frac34}}  {\epsilon^{-\frac14}} 
\]
Taylor length scale
\[
\lambda = \sqrt{  \frac{ <u_1>  } { <u_{1,1}> } } = 10 k \nu / \epsilon
\]
Taylor Reynols number
\[
R_\lambda = \frac{\lambda < u_1 > }{\nu} = k \sqrt{\frac{20}{3 \nu \epsilon}  }
= \sqrt{\frac{20}{3}}  L^{2/3} \eta^{-2/3}
\]
Eddy turnover time
\[
t_e = 2 k / \epsilon
\]

\subsection{Resolution Condition}

Forced turbulence simulations, where one is primarily interested in
the inertial range dynamics, are usually run with a resolution condition
given by
\[
   k_\text{max} \eta \ge 1
\]
The Komogorov length scales gives an estimate of the length scales
where the flow becomes dominated by viscosity effects and the dynamics
beyond those lengths scales do not effect the larger, turbulent length
scales.

More conservative investigators will sometimes require
\[
   k_\text{max} \eta \ge 1.5
\]
or even larger values for special applications.  

This resolution condition determines the amount of viscosity $\nu$
needed.  In practice, one chooses the smallest viscosity possible
viscosity (for a given forcing) so that this resolution condition is
not violated.  






\subsection{CFL based time step}

If the time step used is based on the CFL condition, than at
the end of each timestep one global collective is required 
(of a single real*8 value).  In practice we perform two global collectives
(a sum and a min) of 96 bytes of data.  This is used for the CFL condition
and other diagnostics.  

The usual CFL condition is written as 
\[
\Delta t \le C_0  { \Delta x } / { \text{max} ( \uv ) }
\]
For incompressible homogeneous isotropic turbulence, it is customary
to write this in terms of the kinetic energy $k$, and maximum
wave number $k_\text{max}$,
\[
\Delta t \le C    k^{-1/2} k_\text{max}^{-1}
\]
We have determined, via experiment, that a stable timestep is
given by 
\[
C=0.5.
\]  
As a check on this value, consider that given
in Pope, {\em Turbulent Flows}, Cambridge University Press, 2000.
\[
 \Delta t = \frac1{20} \Delta x k^{-1/2} = .105   k^{-1/2} k_\text{max}^{-1}
\]
where we have assumed a 2/3 dealiasing.  

We write the time step in terms of eddy turnover time by
\[
\Delta t / t_e =  \frac12 C L^{-1}  k_\text{max}^{-1}
\]

\subsection{Deterministic Low Wave Number Forcing}

The deterministic low wave number forcing we use is a version
of Overholt and Pope, Comput. Fluids 27 1998.  It uses a simplified
version of their formula for the relaxation time scale, and only forces
in the first two spherical wave numbers shells.
It is documented in detail in Taylor, Kurien and Eyink, Phys. Rev. E 68, 2003. 

The forcing results in $k=1.9$ and $\epsilon=3.6$, which gives
an eddy turnover time of $t_e = 1.05$ and $L=.73$. 
When using this forcing,
after deciding on the resolution condition $k_\text{max} \eta$,
one can determine the viscosity (which must be specified in the
input file) to use in the simulation based on the resolution and 
type of dealiasing.
Taking
\[
   k_\text{max} \eta = 1
\]
we have $ \nu = \epsilon^{1/3} k_\text{max}^{-4/3}$.
The resulting Taylor Reynolds and CFL condition are
\[
R_\lambda =   \sqrt{\frac{20}{3}} L^{2/3} k_\text{max}^{2/3}  =   2.1 k_\text{max}^{2/3}
\qquad
\Delta t / t_e =    .35 k_\text{max}^{-1}
\]
assuming $C=.5$


\subsection{CFD: Example: $12288^3$ with spherical dealiasing}
Assuming a spherical dealiasing, with resolution condition $k_\text{max} \eta=1$, 
for a problem of size $N=12288 \quad (k_\text{max} = 36396)$, we have
\[
\nu = 1.3 \times 10^{-6} \qquad  R_\lambda =  2300 \qquad \Delta t / t_e = .96\times10^{-5}
\]
100,000 timesteps are required for 1 eddy turnover time.  

\subsection{CFD: Example: $12288^3$ with 2/3 rule dealiasing}
Assuming a spherical dealiasing, with resolution condition $k_\text{max} \eta=1$, 
for a problem of size $N=12288 \quad (k_\text{max} = 25736)$, we have
\[
\nu = 2 \times 10^{-6} \qquad  R_\lambda =  1800\qquad \Delta t / t_e = 1.4\times10^{-5}
\]
71,000 timesteps are required for 1 eddy turnover time.  

\subsection{CFD: Example: $1024^3$ with 2/3 rule dealiasing}
Assuming a spherical dealiasing, with resolution condition $k_\text{max} \eta=1$, 
for a problem of size $N=1024 \quad (k_\text{max} = 3033)$, we have
\[
\nu = 3.5 \times 10^{-5} \qquad  R_\lambda =  440\qquad \Delta t / t_e = 1.2\times10^{-4}
\]
8,333 timesteps are required for 1 eddy turnover time.  




\end{document}
